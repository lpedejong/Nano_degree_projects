{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "This reports aims to analyse the conversion data from a website. Several statistical analyses are used to determine whether a new page design results in more conversions, thus allowing for an advise on the new page design. The analysis consists of descriptive statistics regarding the initial data followed by an A/B test and regression analysis. The regression analysis allows for confirmation of the A/B test results and for the addition of variables into the analysis\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability\n",
    "\n",
    "firstly import the libraries and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#We are setting the seed to assure you get the same answers on quizzes as we set up\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ab = pd.read_csv(\"ab_data.csv\")\n",
    "#show excerpt of dataset to check for succes\n",
    "df_ab.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having imported the dataset, the general characteristics of the data are displayed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294478 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   user_id       294478 non-null  int64 \n",
      " 1   timestamp     294478 non-null  object\n",
      " 2   group         294478 non-null  object\n",
      " 3   landing_page  294478 non-null  object\n",
      " 4   converted     294478 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_ab.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from this we can extract the following metrics:\n",
    "*unique users:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique users: 290584 \n"
     ]
    }
   ],
   "source": [
    "print(\"the number of unique users: %i \" %df_ab[\"user_id\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*the average conversion rate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average conversion rate: 0.119659 \n"
     ]
    }
   ],
   "source": [
    "print('the average conversion rate: %f ' %df_ab['converted'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is clear from the excerpt shown above, that there is a connection between the `group` and `landing_page` columns. If the landing page is `old_page`, the user should be in the group `control`. Therefore it is prudent to check if this is indeed the case for all rows in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting all instances where the landing page is 'new-page' AND group is 'control' and adding this to instances where 'old_page' is in group 'treatment'\n",
    "df_ab.query('landing_page ==\"new_page\" and group == \"control\" ').count()[0] + df_ab.query('landing_page ==\"old_page\" and group == \"treatment\" ').count()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3893 cases, where the landing page does not match the correct group. For these cases it cannot be ascertained which `landing_page` was presented to them. The data is thus unreliable, and these rows are to be deleted. For the sake of efficiency a check on missing values is also performed. The rows with missing values can then be deleted simultaniously.\n",
    "f. Do any of the rows have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         0\n",
       "timestamp       0\n",
       "group           0\n",
       "landing_page    0\n",
       "converted       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for missing values\n",
    "df_ab.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only the rows with unmatchinig *page* and *treatment* values are to be deleted, it is most efficient to compute a new DataFrame specifying the correct relation between `landing_page` and `group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new dataframe\n",
    "df2 = df_ab.query('(landing_page == \"new_page\" and group == \"treatment\") or (landing_page == \"old_page\" and group == \"control\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the correct rows were removed - this should be 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset cleaned, we can continue descriptions of the dataset, first of all checking for all unique users in the dataset, and potential duplications in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting all unqiue user id's\n",
    "df2.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id                   timestamp      group landing_page  converted\n",
      "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0\n"
     ]
    }
   ],
   "source": [
    "#displaying the duplicated rows\n",
    "print(df2[df2.duplicated('user_id')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no further anomolies with the duplicated user, which means this row can be dropped without consequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpede\\anaconda3\\envs\\nanodegree\\lib\\site-packages\\pandas\\core\\frame.py:4160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "#drop the duplicated user ID based on the index\n",
    "df2.drop(index = 2893, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check whether the total number of unique user id's has changed.\n",
    "df2.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if there are only unique users in the dataset by comparing the number of unique user id's to the total nr of records\n",
    "len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the total number of records is equal to the nr of unique user id's, we don't need to implement a filter on unique user id's going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general statistics of the dataset can be extracted from `df2` going forward. Since the goal of this report is to determine if the new page leads to more conversions the following statistics are determined:  \n",
    "`1` the general conversion rate  \n",
    "`2` the conversion rate for the control group  \n",
    "`3` the conversion rate for the treatment group  \n",
    "`4` The chance of being in the treatment group for a new user  \n",
    "`5` The difference in conversion rates between the control and treatment group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#general vonersion rate, stored for future use\n",
    "p = df2.converted.mean()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the conversion rate for the control group, stored for future use\n",
    "p_control = df2.query('group == \"control\"').converted.mean()\n",
    "p_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the conversion rate for the treatment group, stored for future use\n",
    "p_treatment = df2.query('group == \"treatment\"').converted.mean()\n",
    "p_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the chance of recieving a new_page, and thus being in the treatment group.\n",
    "df2[df2['landing_page']== 'new_page'].count()[0]/df2.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0015782389853555567"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the difference in conversion rates between the treatment and control group.\n",
    "obs_diff = p_treatment - p_control\n",
    "obs_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given that the observed difference in conversions between the new page and old page is negative, it would not be prudent to state that the new page leads to more conversions. What is observed here is that the old page has more conversions than the new one. As the total number of users is fairly large with 290000 and the ratio of showing the old page vs new is 50/50, the above observation holds.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "Notice that because of the time stamp associated with each event, you could technically run a hypothesis test continuously as each observation was observed.  \n",
    "\n",
    "However, then the hard question is do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?  How long do you run to render a decision that neither page is better than another?  \n",
    "\n",
    "These questions are the difficult parts associated with A/B tests in general.  \n",
    "\n",
    "\n",
    "First tests include an hypothesis test at a Type I error rate of 5%. The hypotheses are stated in terms of **$p_{old}$** and **$p_{new}$** . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$H_{0}$ : $p_{old} - p_{new} >= 0 $**     \n",
    "\n",
    "**$H_{1}$ : $p_{old} - p_{new} < 0 $**      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test on a singe value, the initial assumption is that under the null hypothesis $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, assume they are equal to the **converted** rate in **ab_data.csv** regardless of the page. This is the value previously saved under `p`. <br><br>\n",
    "\n",
    "In order to test this we simulate samples on a normal distribution assumping `p`, with a sample size equal to **ab_data.csv** and with replacement.  <br><br>\n",
    "\n",
    "From this we generate a sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>\n",
    "The conversion rate `p_new` under the null is the conversion rate as it can be extracted from the dataset. The assumption in this part of the analysis is, that there is **no** difference between the new page or the old page. Therefore we can take the total conversion rate observed in the dataset as the `p_new` **and** as `p_old`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# under the null hypothesis P_new and p_old are the same\n",
    "p_new = df2.converted.mean()\n",
    "p_old = df2.converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to find the conversion rates we need the total nr of users in the different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nr of users in the Treatment group\n",
    "n_new = df2[df2['group']==\"treatment\"].user_id.nunique()\n",
    "n_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nr of users in the control group\n",
    "n_old = df2[df2['group']== \"control\"].user_id.nunique()\n",
    "n_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test whether or not the difference between the conversion rates `p_new` and `p_old` is indeed 0 (as the null hypothesis states), we simulate a random series of conversions, assuming the `p_new` as the chance of conversion from a binomial distribution. As we do the same for `p_old`, the difference between these two distributions should give us a good idea what this difference would be **if** the null hypothesis hold strue. We store the results under `new_page_converted` and `old_page_converted`, and storing the difference between the means of these two arrays under `p_diff_null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_page_converted = np.random.binomial(1,p_new, n_new)\n",
    "new_page_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_page_converted = np.random.binomial(1,p_old,n_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0023282741489399073"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_diff_null = old_page_converted.mean()-new_page_converted.mean()\n",
    "p_diff_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the above simulation that the statistics give a small difference between old and new page conversion rates (`p_diff_null`), this is consistent with the difference found in the data (`obs_diff`). In order to see if we can expect a difference of 0 or larger in most cases, we simulate 10,000 $p_{new}$ - $p_{old}$ values using the same simulation process and store these values in `p_diffs`. Plotting the values in `p_diffs` can give us a visual of the distribution while providing a basis for a z-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the empty list, to be filled by the for loop\n",
    "p_diffs = []\n",
    "#for loop, iterating the simulation of conversions, and adding the resulting difference in conversion ratios to the list.\n",
    "for _ in range(10000):\n",
    "    npc = np.random.binomial(1,p_new, n_new)\n",
    "    opc = np.random.binomial(1,p_old,n_old)\n",
    "    pdiff = opc.mean()-npc.mean()\n",
    "    p_diffs.append(pdiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram of `p_diffs` should show a normal distribution, by indicating the location of the observed difference in the data `obs_diff` in the histogram we can see where it falls in the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x25abc3e99a0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p_diffs);\n",
    "plt.axvline(x= obs_diff, color = 'red' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the null hypothesis states that the difference between old and new page conversion rates is 0 or larger then zero, we are interested in the proportion of values from our simulation that confirm the null hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9036"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#proportion of values in line with the null hypothesis.\n",
    "prop = (p_diffs > obs_diff).mean()\n",
    "prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The null hypothesis states: we expect the difference between conversions with the old page and the new page are larger or equal to one. We see that the proportion of simulated behaviours in the null hypothesis, that are indeed larger than the observed difference is 90%. This means that we have no grounds for rejecting the null hypothesis.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same procedure is also coded in the statsmodels module. Performing a z-test will also give us a value for the proportion in line with the null hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#number of conversions from the old page\n",
    "convert_old = len(df2.query('landing_page == \"old_page\" and converted == 1'))\n",
    "#number of conversions from the new page\n",
    "convert_new = len(df2.query('landing_page == \"new_page\" and converted == 1'))\n",
    "#nr of total users on old page\n",
    "n_old = len(df2.query('landing_page == \"old_page\"'))\n",
    "#nr of total users on new page\n",
    "n_new = len(df2.query('landing_page == \"new_page\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3109241984234394, 0.9050583127590245)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the required statistics and giving the inputs\n",
    "z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new,n_old], alternative = 'larger')\n",
    "z_score,p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The z-score indicates the number of standard deviatons that our observed value is removed from the expected mean under a normal distribution. We see that our observed value is 1.3 standard deviations removed, which is within the 95% confidence interval demanded by the $\\alpha$ of 0.05. The p_value, indicates the chance of finding a value consistent with the null hypothesis, at 0.90 it is larger than the $\\alpha$. Therefore there is, as was with the previous method no grounds for rejecting the null hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "A regression analysis can provide further confirmation and allows for the addition of factors in the analysis. Since each row is either a conversion or no conversion, a logistical regression model is used, including a dummy variable expressing the landing page as a 0 for the old page and a 1 for the new page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **statsmodels** is used to fit a logistical regression model to see if there is a significant difference in conversion based on which page a customer receives. To do this an **intercept** column, as well as an **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control** is added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pandas' from 'C:\\\\Users\\\\lpede\\\\anaconda3\\\\envs\\\\nanodegree\\\\lib\\\\site-packages\\\\pandas\\\\__init__.py'>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-108-da402c208e87>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['intercept'] = 1\n",
      "C:\\Users\\lpede\\anaconda3\\envs\\nanodegree\\lib\\site-packages\\pandas\\core\\frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\lpede\\anaconda3\\envs\\nanodegree\\lib\\site-packages\\pandas\\core\\frame.py:4160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "C:\\Users\\lpede\\anaconda3\\envs\\nanodegree\\lib\\site-packages\\pandas\\core\\frame.py:4293: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page  \n",
       "0          1        0  \n",
       "1          1        0  \n",
       "2          1        1  \n",
       "3          1        1  \n",
       "4          1        0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the intercept columns\n",
    "df2['intercept'] = 1\n",
    "#creating new colums for control and treatment and populating them with dummy values\n",
    "df2[['control','treatment']] = pd.get_dummies(df2['group'])\n",
    "#drop one of the columns, since only one column can be used for the regeression analysis.\n",
    "df2.drop('control', axis = 1, inplace = True)\n",
    "#rename the column to 'ab_page'\n",
    "df2.rename(columns = {'treatment': 'ab_page'}, inplace = True)\n",
    "#check for results\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**statsmodels** is used to instantiate your regression model on the two columns created above, then fitted to predict the conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290582</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 17 Nov 2020</td> <th>  Pseudo R-squ.:     </th>  <td>8.077e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:00:55</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1899</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290582\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Tue, 17 Nov 2020   Pseudo R-squ.:               8.077e-06\n",
       "Time:                        17:00:55   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1899\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = sm.Logit(df2['converted'],df2[['intercept', 'ab_page']])\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9851119396030626, 1.015113064615719)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating the exponent and the inversion as expression of the product increase in chances for conversions\n",
    "np.exp(-0.0150), 1/np.exp(-0.0150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The A/B-test performed earlier, supposes direction in the results. i.e the null hypothesis and alternative are postulated as being equal or greater then or smaller then 0:** \n",
    "\n",
    "**$H_{0}$ : $p_{old} - p_{new} >= 0 $**     \n",
    "\n",
    "**$H_{1}$ : $p_{old} - p_{new} < 0 $**     \n",
    "\n",
    "**In a logistical regression analysis , there is no direction proposed, since the model only observes two possible outcomes. Therefore the either the variable affects the outcome or it does not. The null hypothisis is that the chance of succes (meaning conversion) is equal between the old and new page, there is no relation between the page and the conversion rate. \n",
    "so the null hypothesis in the logistic regression model is:**\n",
    "\n",
    "**$H_{0}$ : $p_{old} = p_{new}$**  \n",
    "**$H_{1}$ : $p_{conversion} \\neq 0$**\n",
    "\n",
    "**the p-value for the logistic model is 0.190, well above 0.05 offering no grounds to reject the null hypothesis. This p-value is larger then the p-value found using the z-test, which was 0.09. This is because the z-tests includes a directional component, reducing the surface of the distribution monitored. However both tests conclude that the null hypothesis should not be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding additional terms might offer further explenaition for the result. However if additional terms are correlated to each other they will interfere with the results. it is therefore important to check any additional terms for multicollinearity. The terms available at the moment are the page that the user lands on, the group they are placed in and their user_id. As is concluded in the introduction to this report, the landing page and group are closely correlated. Since we already utelise the group in our model, adding the landing page, will not improve the prediction. The user_id contains no further information and will also add little to the analysis. \n",
    "\n",
    "From each user, the country of residence is available in a seperate dataset. This information may prove to add to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>834778</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>928468</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>822059</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>711597</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>710616</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id country\n",
       "0   834778      UK\n",
       "1   928468      US\n",
       "2   822059      UK\n",
       "3   711597      UK\n",
       "4   710616      UK"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the country data\n",
    "dfc= pd.read_csv('countries.csv')\n",
    "#view format of the DataFrame\n",
    "dfc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both the `dfc` and `df2` dataframes utelise user_id's the two dataframes can be joined on this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page country  \n",
       "0          1        0      US  \n",
       "1          1        0      US  \n",
       "2          1        1      US  \n",
       "3          1        1      US  \n",
       "4          1        0      US  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joining the dataframes\n",
    "df2_join = df2.join(dfc.set_index('user_id'), on = 'user_id')\n",
    "#check for results\n",
    "df2_join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit a logistical regression model on the data on countries, dummy variables need to be introduced to the dataframe. First of the unique countries present in the dataset are identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UK', 'US', 'CA'], dtype=object)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc['country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three countries are present in the dataset. Below a column is created for each country populated by a dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page country  CA  UK  US  \n",
       "0          1        0      US   0   0   1  \n",
       "1          1        0      US   0   0   1  \n",
       "2          1        1      US   0   0   1  \n",
       "3          1        1      US   0   0   1  \n",
       "4          1        0      US   0   0   1  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_join[['CA','UK','US']] = pd.get_dummies(df2_join['country'])\n",
    "df2_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 17 Nov 2020</td> <th>  Pseudo R-squ.:     </th>  <td>2.323e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:00:57</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1760</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9794</td> <td>    0.013</td> <td> -155.415</td> <td> 0.000</td> <td>   -2.004</td> <td>   -1.954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>   -0.0099</td> <td>    0.013</td> <td>   -0.743</td> <td> 0.457</td> <td>   -0.036</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>   -0.0506</td> <td>    0.028</td> <td>   -1.784</td> <td> 0.074</td> <td>   -0.106</td> <td>    0.005</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Tue, 17 Nov 2020   Pseudo R-squ.:               2.323e-05\n",
       "Time:                        17:00:57   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1760\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9794      0.013   -155.415      0.000      -2.004      -1.954\n",
       "ab_page       -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "US            -0.0099      0.013     -0.743      0.457      -0.036       0.016\n",
       "CA            -0.0506      0.028     -1.784      0.074      -0.106       0.005\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the logistical regression model, utilising two of the three countries to prevent multicollinearity\n",
    "mod = sm.Logit(df2_join['converted'],df2_join[['intercept','ab_page','US','CA']])\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results show that the country, whether the baseline UK or the US or Canada, no statistical difference in the conversion is shown. P values are higher then 0.05 at **0.457** and **0.074** respectively for the US and Canada. It could be possible that there is an interaction between the countries and ab_page. If this is the case we should see differences in the p values, when adding interaction terms for the countries to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the interaction terms\n",
    "df2_join[\"ab_page_us\"] = df2_join[\"US\"]*df2_join['ab_page']\n",
    "df2_join[\"ab_page_uk\"] = df2_join[\"UK\"]*df2_join['ab_page']\n",
    "df2_join[\"ab_page_ca\"] = df2_join[\"CA\"]*df2_join['ab_page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290578</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     5</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 17 Nov 2020</td> <th>  Pseudo R-squ.:     </th>  <td>3.482e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:00:58</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1920</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>   -1.9922</td> <td>    0.016</td> <td> -123.457</td> <td> 0.000</td> <td>   -2.024</td> <td>   -1.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>    <td>   -0.0206</td> <td>    0.014</td> <td>   -1.505</td> <td> 0.132</td> <td>   -0.047</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>         <td>    0.0057</td> <td>    0.019</td> <td>    0.306</td> <td> 0.760</td> <td>   -0.031</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>         <td>   -0.0118</td> <td>    0.040</td> <td>   -0.296</td> <td> 0.767</td> <td>   -0.090</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page_uk</th> <td>    0.0314</td> <td>    0.027</td> <td>    1.181</td> <td> 0.238</td> <td>   -0.021</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page_ca</th> <td>   -0.0469</td> <td>    0.054</td> <td>   -0.872</td> <td> 0.383</td> <td>   -0.152</td> <td>    0.059</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290578\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Tue, 17 Nov 2020   Pseudo R-squ.:               3.482e-05\n",
       "Time:                        17:00:58   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1920\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9922      0.016   -123.457      0.000      -2.024      -1.961\n",
       "ab_page       -0.0206      0.014     -1.505      0.132      -0.047       0.006\n",
       "US             0.0057      0.019      0.306      0.760      -0.031       0.043\n",
       "CA            -0.0118      0.040     -0.296      0.767      -0.090       0.066\n",
       "ab_page_uk     0.0314      0.027      1.181      0.238      -0.021       0.084\n",
       "ab_page_ca    -0.0469      0.054     -0.872      0.383      -0.152       0.059\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the new model and fitting\n",
    "mod = sm.Logit(df2_join['converted'],df2_join[['intercept','ab_page','US','CA','ab_page_uk','ab_page_ca']])\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis shows, no statistical significance in the potential interaction terms. Therefore it can be said that there is no interaction between the countries and the conversion rates. This means that the results of our previous analysis can be applied irregardless of the country of origen of the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "# Conclusions\n",
    "\n",
    "After performing an A/B test, z-test and logistic regression analysis, all results point towards upholding the null hypothesis. This hypothesis being that **there is no improvement to be gained from the new page in terms of conversions made**.\n",
    "\n",
    "The addition of country of origen of the users has not altered this result, and the analysis utilising interaction terms shows that there is no interaction between the countries of origin and the conversion rate. Thus country of origin has no significant effect on the conversion rate. \n",
    "\n",
    "In the logistic regression analysis a product decrease in the chance of conversion with the old page of *1.02* has been found. This result was statistically not significant, but also from a practical standpoint, with such a minor difference no benefit or harm can be found. Per example the general conversion rate `p` was *0.119*. A *1.02* decrease would come to *0.117* conversion rate. \n",
    "\n",
    "The overall conclusion must be that, from the data presented,  there is no reason to move forward with the new design.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
