{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of data\n",
    "\n",
    "Data was loaded from 3 sources\n",
    "\n",
    "1. .csv file downloaded manually from https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv\n",
    "2. .tsv file downloaded programmatically from https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "3. .txt file gathered through Twitter API using the Tweepy module\n",
    "\n",
    "The following code was used to load and or gather the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv file\n",
    "twitter_archive = pd.read_csv(\"twitter-archive-enhanced-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_conf</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>666020888022790149</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Welsh_springer_spaniel</td>\n",
       "      <td>0.465074</td>\n",
       "      <td>True</td>\n",
       "      <td>collie</td>\n",
       "      <td>0.156665</td>\n",
       "      <td>True</td>\n",
       "      <td>Shetland_sheepdog</td>\n",
       "      <td>0.061428</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>666029285002620928</td>\n",
       "      <td>https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>redbone</td>\n",
       "      <td>0.506826</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.074192</td>\n",
       "      <td>True</td>\n",
       "      <td>Rhodesian_ridgeback</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>666033412701032449</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>German_shepherd</td>\n",
       "      <td>0.596461</td>\n",
       "      <td>True</td>\n",
       "      <td>malinois</td>\n",
       "      <td>0.138584</td>\n",
       "      <td>True</td>\n",
       "      <td>bloodhound</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>666044226329800704</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5Dr8HUEAA-lEu.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Rhodesian_ridgeback</td>\n",
       "      <td>0.408143</td>\n",
       "      <td>True</td>\n",
       "      <td>redbone</td>\n",
       "      <td>0.360687</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>666049248165822465</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5IQmsXIAAKY4A.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.560311</td>\n",
       "      <td>True</td>\n",
       "      <td>Rottweiler</td>\n",
       "      <td>0.243682</td>\n",
       "      <td>True</td>\n",
       "      <td>Doberman</td>\n",
       "      <td>0.154629</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                          jpg_url  \\\n",
       "0  666020888022790149  https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg   \n",
       "1  666029285002620928  https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg   \n",
       "2  666033412701032449  https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg   \n",
       "3  666044226329800704  https://pbs.twimg.com/media/CT5Dr8HUEAA-lEu.jpg   \n",
       "4  666049248165822465  https://pbs.twimg.com/media/CT5IQmsXIAAKY4A.jpg   \n",
       "\n",
       "   img_num                      p1   p1_conf  p1_dog                  p2  \\\n",
       "0        1  Welsh_springer_spaniel  0.465074    True              collie   \n",
       "1        1                 redbone  0.506826    True  miniature_pinscher   \n",
       "2        1         German_shepherd  0.596461    True            malinois   \n",
       "3        1     Rhodesian_ridgeback  0.408143    True             redbone   \n",
       "4        1      miniature_pinscher  0.560311    True          Rottweiler   \n",
       "\n",
       "    p2_conf  p2_dog                   p3   p3_conf  p3_dog  \n",
       "0  0.156665    True    Shetland_sheepdog  0.061428    True  \n",
       "1  0.074192    True  Rhodesian_ridgeback  0.072010    True  \n",
       "2  0.138584    True           bloodhound  0.116197    True  \n",
       "3  0.360687    True   miniature_pinscher  0.222752    True  \n",
       "4  0.243682    True             Doberman  0.154629    True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load additional modules\n",
    "import requests\n",
    "import os\n",
    "# download the image predictions file\n",
    "# commented out for repeat running of code\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "#download dog breed predictions file and store in base directory\n",
    "with open('predictions.tsv',mode = 'wb') as file:\n",
    "    file.write(response.content)\n",
    "# open file\n",
    "predictions = pd.read_csv('predictions.tsv', delimiter = '\\t' )\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json module\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "# gather tweet data through twitter API and store in .txt file\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "tweet_ids = predictions.tweet_id.values\n",
    "\n",
    "with open ('tweet_json3.txt', 'w', encoding = \"utf-8\") as outfile:\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode= 'extended', wait_on_rate_limit = True, wait_on_rate_limit_notify = True)\n",
    "            print(\"succes\")\n",
    "            json.dump(tweet._json, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "#load .txt file and store in Data Frame\n",
    "tweets = []\n",
    "with open( 'tweet_json3.txt', 'r', encoding = \"utf-8\") as file:\n",
    "    for line in file:\n",
    "        tweet = json.loads(line)\n",
    "        tweet_id = tweet['id']\n",
    "        retweet_count = tweet['retweet_count']\n",
    "        favorite_count = tweet['favorite_count']\n",
    "        tweets.append({'tweet_id': tweet_id, 'retweet_count': retweet_count, 'favorite_count': favorite_count})\n",
    "api_twitter_counts = pd.DataFrame(data= tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assesment\n",
    "\n",
    "Each individual file was visually and programmatically assessed \n",
    "\n",
    "## Visual assesments\n",
    "\n",
    "The `twitter_archive_enhanced.csv` file was visually assesed but no issues where identified from this assesment. The `predictions.tsv` was visually assesed, from this assesment it was concluded that several rows contained no correct predictions of dog breeds. The `tweet_json.txt` file was not visually assesed.\n",
    "twitter_archive_clean.info()\n",
    "\n",
    "## programmatic assesment\n",
    "Programmatic assesment contains of initially utlising the `pd.info()` command to check for `NaN` values. Furthermore checks are performed on different columns to see if they contain expected data forms using regex functions like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check tweet_id to see if the tweet_id column contains a sequence of digits or not\n",
    "twitter_archive_clean[~twitter_archive_clean['tweet_id'].astype(str).str.contains(r'\\d{18}')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further columns are also checked for the following expected values in the same manner:\n",
    "* timestamp column is expected to contain data in the same time format for each rows\n",
    "* rating numerator and rating denominator are expected to be one or two digit numbers\n",
    "* The name column is expected to be a single word\n",
    "Further checks are performed to see if columns contain unexpected or unwanted values that are not found by the regex operation\n",
    "* the name column is not expected to contain the string 'None' \n",
    "* the \"doggo\", \"floofer\", \"pupper\" and \"puppo\" columns are expected to contain only these respective strings and not the string \"none\"\n",
    "Checking for all the unique values in different columns can give us an idea of further data quality issues\n",
    "* The source column may contain many unique items\n",
    "* the \"doggo\", \"floofer\", \"pupper\" and \"puppo\" columns are expected to contain no unique items besides the respective strings and the string \"none\"4\n",
    "* the different favorite and retweet id columns are expected to only contain id's or NaN values\n",
    "\n",
    "The programmatic assesment continued with the assesment of the predictions file\n",
    "* Confirming visual assesment that some tweets contain no accurate prediction of dog breed\n",
    "\n",
    "## Issues\n",
    "Through the above described assesment the following issues are identified:\n",
    "### Data tidyness\n",
    "#### twitter archive .csv file\n",
    "     1. `doggo`, `floofer`, `pupper` and `puppo` are mutually exclusive, should be a single column\n",
    "\n",
    "#### dog race predictions .tsv file\n",
    "     2. Correct predictions for dog breed are not presented in a single column\n",
    "     3.the predictions are required for final analysis, and therefore should be added to the twitter archive DataFrame\n",
    "#### tweet_json.txt file\n",
    "     4. the api_twitter_counts dataframe needs to be added to the twitter_archive_clean dataframe\n",
    "### Data quality\n",
    "#### twitter archive .csv file\n",
    "     1.The `name` column contains 745 rows with the string \"None\" instead of a name\n",
    "     2.The new single `doggo`,`floofer`,`pupper`,`puppo` column contains rows with the string \"None\" \n",
    "     3.The alternate id columns contain mostly NaN values \n",
    "     4.The `retweeted_status_timestamp` column contains mostly NaN values\n",
    "     5.The alternate id columns data when non NaN values are float, should be integer or string\n",
    "     6.`timestamp` should be in datatime format\n",
    "     10.After creating the `dog_type` column to solve tidyness issue 1 and quality issue 2, it contains a lot of NaN values\n",
    "\n",
    "#### dog race predictions .tsv file\n",
    "     7.The entire dataset contains 2075 entries as opposed to 2256 entries for twitter archive\n",
    "     8.324 tweets have no positive recognition of dog breed, resulting in missing data\n",
    "#### tweet_json file\n",
    "     9.resulting dataframe has 2075 entries as opposed to 2256 entries for twitter archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "Cleaning commences with the completeness issues identified. These being the **quality issues 3 and 4**. These issues are cleaned by first removing the rows with non-NaN values. These rows are replies or retweets and can be removed, as per the assignment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check total number of rows to be deleted\n",
    "len(twitter_archive_clean.query('in_reply_to_status_id.notnull() or in_reply_to_user_id.notnull() or retweeted_status_id.notnull() or retweeted_status_user_id.notnull()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find rows with no nan values\n",
    "lstid1 = twitter_archive_clean[twitter_archive_clean.in_reply_to_status_id.notnull()].index.values.tolist()\n",
    "lstid2 = twitter_archive_clean[twitter_archive_clean.in_reply_to_user_id.notnull()].index.values.tolist()\n",
    "lstid3 = twitter_archive_clean[twitter_archive_clean.retweeted_status_id.notnull()].index.values.tolist()\n",
    "lstid4 = twitter_archive_clean[twitter_archive_clean.retweeted_status_user_id.notnull()].index.values.tolist()\n",
    "# combine lists\n",
    "lst_id_temp= lstid1 + lstid2 + lstid3 + lstid4\n",
    "# remove duplicates by converting to dictionary and back to list\n",
    "dic = dict.fromkeys(lst_id_temp)\n",
    "#convert back to list\n",
    "lst_id = list(dic)\n",
    "#check length of list\n",
    "len(lst_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with retweeted tweets\n",
    "twitter_archive_clean.drop(lst_id, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing these rows, the columns 'in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp' can be dropped as these rows play  no role in further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean.drop(columns = ['in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning effort continuous with tidyness issues\n",
    "## Tidyness\n",
    "The combining of the `doggo`, `floofer`, `pupper` and `puppo` columns is done by replacing the string `None` with `NaN`. Then nesting a fillna command for all four columns and filling a copy of the `doggo` column with `floofer`, `pupper` and `puppo` values respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean['dog_type'] =twitter_archive_clean[\"doggo\"].fillna(twitter_archive_clean[\"floofer\"].fillna(twitter_archive_clean[\"pupper\"].fillna(twitter_archive_clean[\"puppo\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solves tidyness **issue 1** and also **quality issue 2**.\n",
    "\n",
    "**tidyness issue 2 and 3** are cleaned by sourcing data from several columns in the `predictions` dataframe, and copying this in a new column called `breed`. This column is subsequently added to the `twitter_archive_clean` DataFrame. The desired columns from the `predictions` dataframe is merged to the `twitter_archive_clean` dataframe using an left join. This way only additional information on the tweets win the `twitter_archive_clean` dataframe is added. This solves **quality issue 7**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution to tidyness issue 2\n",
    "comparison = np.where(predictions[\"p1_dog\"] == True, predictions['p1'], np.where(predictions['p2_dog']== True, predictions['p2'], np.where(predictions['p3_dog']== True, predictions['p3'],\"unknown\")))\n",
    "# add column to the original dataframe\n",
    "predictions[\"breed\"] = comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retweet and favorite count data is added to the `twitter_archive_clean` dataframe by an inner join, as to only retain tweets that have information in both datasets. This solves **tidyness issue 4** and **quality issue 9**\n",
    "\n",
    "## Quality\n",
    "\n",
    "The remaining quality issues are as follows:\n",
    "#### twitter archive .csv file\n",
    "     1.The `name` column contains 745 rows with the string \"None\" instead of a name\n",
    "     6.`timestamp` should be in datatime format\n",
    "     10.After creating the `dog_type` column to solve tidyness issue 1 and quality issue 2, it contains a lot of NaN values\n",
    "#### dog race predictions .tsv file\n",
    "     8.324 tweets have no positive recognition of dog breed, resulting in missing data\n",
    "\n",
    "**quality issue 1** is cleaned by removal of the `name` column. As no analysis is planned using this information. **quality issue 6** is resolved by changing the datatype for the `timestamp` column by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean = twitter_archive_clean.astype({'timestamp' : 'datetime64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**quality issue 10** is addressed by scanning for additional occurences of the strings `doggo`, `floofer`, `pupper` and `puppo` in the `text` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean['dog_type2'] = twitter_archive_clean['text'].str.extract('(doggo|floofer|pupper|puppo)', expand = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These occurences are added to a new column `dog_type2` as the missing values are filled in as `NaN`, the same tactic as before can be utilised to fill in the additional data to the `dog_type` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean['dog_type']= twitter_archive_clean.dog_type.fillna(twitter_archive_clean.dog_type2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**quality issue 8** is accepted, since there is no way to programmatically asses the missing data within the scope of this project. Since over a 1000 tweets do have a accurate prediction of dog breed, future analysis can be performed.\n",
    "\n",
    "The cleaned data is stored as a .csv file as `twitter_archive_master.csv`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nanodegree] *",
   "language": "python",
   "name": "conda-env-nanodegree-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
